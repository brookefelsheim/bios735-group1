---
title: "Final Report"
author: "Group 1: Jiawen Chen, Brooke Felsheim, Elena Kharitonova, Xinjie Qian, and Jiarui Tang"
date: "4/30/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include=FALSE}
library(knitr)
library(devtools)
library(caret)
library(tidyverse)
library(egg)
library(data.table)
library(randomForest)
library(partykit)
```


# Introduction

As human populations are rising across the world, so is the proportion of people that live in urban areas. Estimates from the *UN World Urbanization Prospects* (2018) indicate that over 4.2 billion people (55% of the global population) currently live in urban areas, and by 2050, an additional 2.5 billion people (68% of the global population) could be living in urban areas. More people living in urban areas calls for more space-, cost-, and energy-efficient systems of transportation as an alternative to cars. One such promising transportation alternative is the implementation of bicycle sharing programs.     

Bicycle sharing programs are transportation schemes that allow individuals to rent bicycles on a short-term basis for either a set rate or for free. Most bicycle sharing programs have many computer-controlled bicycle rack ``hubs" dispersed across a city that keep bikes locked and release them for use when a user enters the appropriate information/payment from a station or an app (Figure 1). A user can then ride the bike and return it to any other bicycle hub that is part of the same program. Many cities across the world have begun implementing bicycle sharing programs, including Chapel Hill, which has a Tar Heel Bikes sharing system^[https://move.unc.edu/bike/bikeshare/]. Systems like these provide convenient, inexpensive, and eco-friendly transportation options for individuals residing in a city.    
```{r bike-img, fig.align = 'center', out.width = "50%", fig.cap = "A `hub' of bicycles belonging to the Santander Cycles system in London. SOPA Images/Lightrocket via Getty Images"}
include_graphics("report_files/santander_bikes.jpg")
```
Successful implementations of bike sharing programs depend on proper management of these systems. It is important for a bike sharing program to provide a stable supply of rental bikes to its population so its users feel that they can rely on the system for their transportation needs. The analysis of bike sharing data allows for a better understanding of the demand of rental bikes in a city, which, in turn, can help inform a city about how to provide appropriate supplies of rental bikes for its population.
For our project, we were interested in predicting the number of bikes rented within a given bike sharing system given information about weather, time of day, and date. We were also interested in assessing the most important variables for predicting bike rental counts. To answer these questions, we fit and evaluated a negative binomial generalized mixed model, a conditional inference tree, and a random forest model, using data from three publicly available bike sharing demand datasets.      
The first dataset we use is a London bike sharing demand dataset downloaded from Kaggle^[https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset] and provided by Transport for London^[https://cycling.data.tfl.gov.uk]. This dataset contains hourly bike rental count observations over two years, from Jan 04 2015 - Jan 03 2017. The first full consecutive year of data was used as the training set in the analysis, and the second full consecutive year of data was held out as a test set.     
The second dataset we use is a Seoul bike sharing demand dataset downloaded from the UCI Machine Learning Repository^[https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand] and provided by the Seoul Metropolitan Government^[https://data.seoul.go.kr]. This dataset contains hourly bike rental counts over one year, from Dec 1 2017 - Nov 30 2018. This was used as an independent test set.     
The third dataset we use is a Washington, D.C. bike sharing demand dataset downloaded from Kaggle^[https://www.kaggle.com/datasets/marklvl/bike-sharing-dataset] and provided by Capital Bikeshare^[https://ride.capitalbikeshare.com/system-data]. This dataset contains hourly bike rental counts over two years, from Jan 01 2011 - Dec 31 2012. This was used as an independent test set.     
Each dataset contained hourly observations of bike rental count data. To simplify our analysis, we chunked the hourly data into three time blocks: [0:00 - 8:00), [8:00 - 16:00), and [16:00 - 24:00). Additionally, because temperature and humidity can be correlated with time of day, we chose to use the maximum and minimum daily temperature and humidity measurements for each 8-hour data point in order to avoid any issues of colinearity.     
We created an R package named `bikeSharing` that includes methods for training and evaluating the negative binomial glmm and random forest models, as well as the processed source data from all three sets.     
The below code can be used to install the package and load the package library. The zipped package source data, `bikeSharing_1.0.0.tar.gz` can be found in the Github repository for our project^[https://github.com/brookefelsheim/bios735-group1].     
```{r echo=TRUE}
if(!require("bikeSharing", quietly = TRUE))
  install.packages("package/bikeSharing_1.0.0.tar.gz", repos = NULL)
library(bikeSharing)
```

Once the package is loaded, the bike sharing data from all three sets becomes easily accessible through the variable names `london`, `seoul`, and `dc`. To directly load them into the R environment, one can simply run:      

```{r echo=TRUE}
data("london", "seoul", "dc")
```

All three datasets contain bike rental count data as well as 11 additional weather-, time-, and date-related variables that were used as predictors in our models:     


  * Hour chunk (00:00 - 8:00, 8:00-16:00, 16:00-24:00)
  * Weekend status (Yes/No)
  * Holiday status (Yes/No)
  * Season (Winter, Spring, Summer, Autumn)
  * Minimum daily temperature (C)
  * Maximum daily temperature (C)
  * Minimum daily humidity (%)
  * Maximum daily humidity (%)
  * Wind speed (m/s)
  * Presence of any rain or snow (Yes/No)
  * Date (mm-dd)

The way that these variables are used within our models will be further described in the Methods section. For all of the analyses performed, the `london` dataset was divided into training and testing sets, where the training set contained all "Year 1" data (Jan 04 2015 - Jan 03 2016), and the testing set contained all "Year 2" data (Jan 04 2016 - Jan 03 2017).

```{r echo=TRUE}
london_train <- london[london$Year == "Year 1",]
london_test <- london[london$Year == "Year 2",]
```


# Methods

## Negative Binomial Generalized Linear Mixed Model

Let $y_{ij}$ be the number of bikes rented at hour chunk $j$ of day $i$. Thus $i$ ranges from 1 to 365, and $j$ ranges from 1 to 3, corresponding to hour chunks [00:00 - 8:00, 8:00-16:00, 16:00-24:00]. We assume that the number of bikes rented for a given hour chunk within a specific day follows a negative binomial distribution, so $Y_{ij} \sim NB(\mu_{ij}, \theta)$ using the Hilbe parameterization, so:

\begin{equation}
P(Y_{ij} = y_{ij}|\mu_{ij}, \theta) =  \frac{\Gamma(y_{ij} + \theta)}{\Gamma(y_{ij} + 1)\Gamma(\theta)} \left( \frac{\theta}{\theta + \mu_{ij}} \right)^{\theta} \left(\frac{\mu_{ij}}{\theta +  \mu_{ij}} \right)^{y_{ij}}
\end{equation}

The mean of $y_{ij}$ for each day $i$ at hour chunk $j$ is $\mu_{ij}$, which we assume follows a negative binomial generalized linear mixed model with a random intercept. Thus it is determined from the following model: 

\begin{equation}
    \log(\mu_{ij}) = x_{ij}^T \beta   + b_i 
\end{equation}

Let $x_{ij}$ = (1,
I(HourChunk$_{ij}$ = [8,16)),
I(HourChunk$_{ij}$ = [16,24)),
Weekend$_{i}$,
Holiday$_{i}$,
I(Season$_{i}$ = Spring),
I(Season$_{i}$ = Summer),
I(Season$_{i}$ = Winter),
Min_Temperature$_{i}$,
Max_Temperature$_{i}$,
Min_Humidity$_{i}$,
Max_Humidity$_{i}$,
Wind_Speed$_{ij}$,
Rain_or_Snow$_{ij})^T$.

HourChunk$_{ij}$ is a categorical variable corresponding to the hour chunk $j$ of day $i$, with the reference hour chunk being [0:00,8:00), Weekend$_{i}$ is a binary variable with 1 if day $i$ is a weekend, 0 if it is not, 
Holiday$_{i}$ is a binary variable with 1 if day $i$ is a holiday, 0 if it is not. Season$_{i}$ is a categorical variable corresponding to which season day $i$ is in, with the reference season being Autumn. The Min_Temperature$_{i}$ and Max_Temperature$_{i}$ are the minimum and maximum temperature of day $i$, respectively. Similarly, 
The Min_Humidity$_{i}$ and Max_Humidity$_{i}$ are the minimum and maximum humidity of day $i$, respectively. Wind_Speed$_{ij}$ is the average wind speed of hour chunk $j$ of day $i$. Rain_or_Snow$_{i}$ is a binary variable with 1 if during day $i$ hour chunk $j$ there is any rain or snow, 0 if there is not. Thus $\beta = (\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6, \beta_7, \beta_8,\beta_9, \beta_{10}, \beta_{11}, \beta_{12}, \beta_{13})'$.  

In Equation (2), $b_i$ is the unobserved random effect that the day $i$ has on the number of rented bikes at any given hour chunk. It is assumed that $b_i \sim N(0,\sigma^2_\gamma)$.


Thus, of interest is the estimate $\theta$, $\beta$, and $\sigma^2_\gamma$. Equations 1 and 2 can be combined to obtain the following likelihood equation for a given day ($i$). 

\begin{equation}
    L(\theta, \beta, \sigma^2_\gamma| \mathbf{y}, \mathbf{b}) =  \prod_{i=1}^{365} \prod_{j=1}^{3} p(y_{ij})  = 
    \prod_{i=1}^{365} \left( \prod_{j=1}^{3} f(y_{ij}|\mu_{ij}) \right) f(b_i | \sigma^2_\gamma) 
\end{equation}

Since the random effects $b_i$ are unobservable, this means they must be integrated out of the above expression to obtain the likelihood, so:


\[  L(\theta, \beta, \sigma^2_\gamma| \mathbf{y}, \mathbf{b})  =  \prod_{i=1}^{365} \left[ \int \left( \prod_{j=1}^{3} f(y_{ij}|\mu_{ij}) \right) f(b_i | \sigma^2_\gamma) \hspace{3 pt} db_i \right] \]

\[   = \prod_{i=1}^{365} \left[ \int \prod_{j=1}^{3} \frac{\Gamma(y_{ij} + \theta)}{\Gamma(y_{ij} + 1)\Gamma(\theta)} \left( \frac{\theta}{\theta +  e^{x_{ij}^T \beta + b_i}} \right)^{\theta} \left(\frac{  e^{x_{ij}^T \beta + b_i}}{\theta +  e^{x_{ij}^T \beta + b_i}} \right)^{y_{ij}} \frac{1}{\sqrt{2 \pi \sigma^2_\gamma}} \exp\left(-\frac{b_i^2}{2\sigma^2}\right) db_i \right]   \]

So thus, the log likelihood is found to be

\[
    l(\theta, \beta, \sigma^2_\gamma) = \sum_{i=1}^{365} log \bigg[ \int \prod_{j=1}^{3} \frac{\Gamma(y_{ij} + \theta)}{\Gamma(y_{ij} + 1)\Gamma(\theta)} 
    \left( \frac{\theta}{\theta +  e^{x_{ij}^T \beta + b_i}} \right)^{\theta} \cdot \\
    \left(\frac{e^{x_{ij} \beta + b_i}}{\theta +  e^{x_{ij} \beta + b_i}} \right)^{y_{ij}} \frac{1}{\sqrt{2 \pi \sigma^2_\gamma}} \exp\left(-\frac{b_i^2}{2\sigma^2_\gamma} \right)
    db_i
    \bigg] 
    \]

This log-likelihood will be maximized to obtain estimates for $\theta, \beta, \sigma^2_\gamma$ through an MCEM approach. Assuming that the $b_i$'s were known, we first define the complete data log likelihood as: 

\[ \log  L_C(\theta, \beta, \sigma^2_\gamma| \mathbf{y}, \mathbf{b})  =  \log \left[ \prod_{i=1}^{365} \left( \prod_{j=1}^{3} f(y_{ij}|\mu_{ij})  f(b_i | \sigma^2_\gamma) \right) \right]  = \sum_{i=1}^{365}\log  \left( \prod_{j=1}^{3} f(y_{ij}|\mu_{ij})  f(b_i | \sigma^2_\gamma) \right) \]

\[ \textrm{So, } l_C(\theta, \beta, \sigma^2_\gamma| \mathbf{y}, \mathbf{b})    = \sum_{i=1}^{365} \left( \sum_{j=1}^{3} \log f(y_{ij}|\mu_{ij})  + \log f(b_i | \sigma^2_\gamma) \right) \]

Thus, for the MCEM algorithm, we will be maximizing the expectation of the complete data log likelihood, otherwise known as the Q-function at step $t$. So the Q-function is defined as the following:


\[Q(\theta, \beta, \sigma^2_\gamma|y, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)}) = E[l_C(\theta, \beta, \sigma^2_\gamma| \mathbf{y}, \mathbf{b})|y, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)}]  \]
\[ = E\left[ \sum_{i=1}^{365} \left( \sum_{j=1}^{3} \log f(y_{ij}|\mu_{ij}^{(t)})  + \log f(b_i | {\sigma^2_\gamma}^{(t)}) \right) \right] \]

\[
=  \sum_{i=1}^{365} \left[ \int \left( \sum_{j=1}^{3} \log f(y_{ij}|\mu_{ij}^{(t)})  + \log f(b_i | {\sigma^2_\gamma}^{(t)}) f(b_i |y, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)} \right) db_i \right]
 \]
 
Where ${ \mu_{ij}^{(t)} = e^{ x_{ij}^T \beta^{(t)}   + b_i  } }$ and ${f(b_i |y, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)} )}$ is  the posterior distribution of $b_i$ given the observed data and the current parameter estimates.  Let $Y_i=(y_{i1},y_{i2},y_{i3})^T$. The density function for $b_i$ given $Y_i, \beta^{(t)}, \theta^{(t)}, {\sigma^2_\gamma}^{(t)}$ is:

\begin{equation}
\begin{split} 
    f(b_i | Y_i,\theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)}) \propto f(Y_i | b_i, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)})\cdot f(b_i|{\sigma^2_\gamma}^{(t)}) \\ \propto \prod_{j=1}^{3} \bigg(\frac{\theta^{(t)}}{\mu_{ij}^{(t)}+\theta^{(t)}}\bigg)^{\theta^{(t)}}\cdot \bigg(\frac{\mu_{ij}^{(t)}}{\mu_{ij}^{(t)}+\theta^{(t)}}\bigg)^{y_{ij}}\cdot \exp\bigg(-\frac{b_i^2}{2{\sigma^2_\gamma}^{(t)}}\bigg) \\
    \propto \sum_{j=1}^{3} \bigg[ (y_{ij}\log\mu_{ij}^{(t)}-(y_{ij}+\theta^{(t)})\log(\mu_{ij}^{(t)}+\theta^{(t)}))-\frac{b_i^2}{2{\sigma^2_\gamma}^{(t)}}\bigg]
\end{split}
\end{equation}

If we could sample from this posterior distribution of $b_i$, we could approximate this integral using a montecarlo approach. So,


\[Q(\theta, \beta, \sigma^2_\gamma|y, \theta^{(t)}, \beta^{(t)}, {\sigma^2_\gamma}^{(t)}) 
=  \frac{1}{M}\sum_{i=1}^{365} \sum_{k=1}^M   \left( \sum_{j=1}^{3} \log f(y_{ij}|\mu_{ijk}^{(t)})  + \log f(b_{ik} | {\sigma^2_\gamma}^{(t)}) \right)
 \]

Where $b_{ik}$ is one of the $M$ samples of the posterior distribution $b_i$ and ${ \mu_{ijk}^{(t)} = e^{ x_{ij}^T \beta^{(t)}   + b_{ik}  } }$.

To sample from this distribution posterior distribution of $b_i$, we employed the metropolis hastings algorithm with a random walk. So at set $t$, for each new $b_i$, we considered $b_i^* = b_i^{(t)} + \epsilon$ where $\epsilon \sim U\left(-\frac{1}{4}, \frac{1}{4}\right)$. From this, the Metropolis Hastings ratio $R(b_i^{(t)}, b_i^*)$ was determined whether or not to accept this new $b_i^*$. For each new $b_I^*$

\[R(b_i^{(t)}, b_i^*) = \frac{f(y_{ij}|\theta, \beta, \sigma^2_\gamma,b_i^*)f(b_i^* | \sigma^2_\gamma)}{f(y_{ij}|\theta, \beta, \sigma^2_\gamma,b_i^{(t)})f(b_i^{(t)} | \sigma^2_\gamma)}\]

Thus, the new value for the $b_i^{(t+1)}$ is: 

\begin{equation*}
b_i^{(t+1)} = \begin{cases}
b_i^* &\text{ with probability min$(R(b_i^{(t)},b_i^*),1)$}\\
b_i^{(t)} &\text{ otherwise}
\end{cases}
\end{equation*}

So in summary, our Metropolis Hastings Algorithm with a random walk is as follows: 
Step 1: Let $b_i^{*}$ be generated by  $b_i^{*}= b^{(t)}+ \epsilon$, where $\epsilon\sim U(-1/4,1/4)$. .
Step 2: Compute the Metropolis–Hastings ratio $R(b_i^{(t)},b_i^{*})$
 where \[R(b_i^{(t)}, b_i^*) = \frac{f(y_{ij}|\theta, \beta, \sigma^2_\gamma,b_i^*)f(b_i^* | \sigma^2_\gamma)}{f(y_{ij}|\theta, \beta, \sigma^2_\gamma,b_i^{(t)})f(b_i^{(t)} | \sigma^2_\gamma)}\]
Step 3: Pick the new value for $b_i^{(t+1)}$ such that
 $$
 b_i^{(t+1)}=\left\{   \begin{aligned}
    b_i^{*} &\text{with probability} \min(R(b_i^{(t)},b_i^{*}),1)   \\
    b_i^{(t)} &\text{otherwise}.
    \end{aligned}\right.
 $$
Step 4: Set t=t+1 and return to step 1.

For the M-step, we will maximize the Q-function with respect to ${\theta, \beta, \sigma^2_\gamma}$. The Nelder-Mead method was used to optimize all of these parameters, this was because Nelder-Mead in the M-step does not require 1st or 2nd derivatives and it is robust.
After this, the E-step is repeated for the next iteration, followed by the M-step again until the estimates for ${\theta, \beta, \sigma^2_\gamma}$ converge.

In our package `bikeSharing`, we have created a function `MCEM_algorithm` that will fit a
Negative Binomial Generalized Linear Mixed Model to the data using the MCEM algorithm described above. 
```{r eval=FALSE, include=TRUE, echo=TRUE}
glmm_fit <- MCEM_algorithm( beta_initial = c(8.3, 1.5, 1.5, -0.25, -0.50, 0, 
                                             0, -0.25, 0, 0, 0, 0, 0, -0.25),
                         theta_initial = 10,
                         s2gamma_initial = 0.2,
                         M = 1000,
                         burn.in = 200,
                         tol = 10^-4,
                         maxit = 100,
                         data = london_train
                         )
```


```{r echo=TRUE}
str(glmm_fit)
```



## Machine learning models

Next, we applied two machine learning methods to predict the bike count. The first model we used is a conditional inference tree. In traditional partitioning, all possible splits are investigated to find the best split, which results in overfitting and selection. The conditional inference tree embeds the partition step with a permutation test, thereby enabling this method to be robust to covariates of different scales. Furthermore, it is capable of stopping when no significant correlation exists between the covariates and the response (Hothorn, Hornik, and Zeileis 2006). We emplyed the ctree funciton in the partykit to fit the conditional inference tree.

We visualized the three layer conditional inference tree in order to verify the relationship between bike count and variables (Figure 2). It is apparent that people tend to rent fewer bikes between midnight and 8 am. Maximum temperatures and humidity also affect bike rental rates. The splits that are employed in the conditional reference tree provide well reasoned explanations of the data structure, which means high accuracy predictions are made.

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.width=5,fig.height=3,fig.align='center',fig.cap="Three layer conditional inference tree"}
# conditional inference tree
train_factor = london_train %>%
  mutate(Date=as.numeric(as.factor(Date)),
         Hour_chunks=as.factor(Hour_chunks),
         Is_weekend=as.factor(Is_weekend),
         Season=as.factor(Season),
         Is_holiday=as.factor(Is_holiday),
         Rain_or_snow=as.factor(Rain_or_snow))
  
test_factor = london_test %>%
  filter(Date!="02-29")%>%
  mutate(Date=as.numeric(as.factor(Date)),
         Hour_chunks=as.factor(Hour_chunks),
         Is_weekend=as.factor(Is_weekend),
         Season=as.factor(Season),
         Is_holiday=as.factor(Is_holiday),
         Rain_or_snow=as.factor(Rain_or_snow))
set.seed(735)
ctree0=ctree(Bike_count ~ Hour_chunks + Min_temp + Max_temp + Min_humidity + Max_humidity + Wind_speed +
                         Rain_or_snow + Is_weekend + Season + Is_holiday + Date,  
             data = train_factor,control=ctree_control(maxdepth = 3))
plot(ctree0,gp = gpar(fontsize = 6,font=2),     # font size changed to 6
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = FALSE))
set.seed(735)
ctree0=ctree(Bike_count ~ Hour_chunks + Min_temp + Max_temp + Min_humidity + Max_humidity + Wind_speed +
                         Rain_or_snow + Is_weekend + Season + Is_holiday+Date,  
             data = train_factor)
ctree_mda=varimp(ctree0)
ctree_mda=data.table(var=names(ctree_mda),mda=ctree_mda)
setorder(ctree_mda,-mda)
colnames(ctree_mda)=c("Variable","Mean decrease in accuracy")
```

A random forest model was then applied to predict the number of bikes that would be rented. Random forest is composed of many decision trees as opposed to a single tree, resulting in a more accurate result. Incorporating the randomness allows random forest to protect against overfitting and can be applied more effectively to other data sets. The disadvantage of the random forest is its computational complexity. To fit the random forest, we created a method `train_random_forest()` within our `bikeSharing` R package that leverages the the `train()` function within the `caret` R package. The optimal tuning parameter mtry was determined using a 5-fold cross validation. We visualized a random selected tree. 

```{r}
set.seed(735)
```

```{r echo = TRUE}
rf_fit <- train_random_forest(data = london_train)
```


```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.width=5,fig.height=4,fig.align='center',fig.cap="A random selected tree in the trained random forest model"}
set.seed(735)
model1 <- randomForest(Bike_count ~ Hour_chunks + Min_temp + Max_temp + Min_humidity + Max_humidity + Wind_speed+Rain_or_snow + Is_weekend + Season + Is_holiday+Date,data = train_factor, importance=TRUE, ntree=500,mtry = 7)

#devtools::install_github('araastat/reprtree')
library(reprtree)

reprtree:::plot.getTree(model1,k=sample(1:500,1),depth=4,cex=1,font=2)
```

Random forest sample trees share similar splits with conditional inference trees (Figure 3). The estimated bike rental count is much higher in the hour chunks that are not 0-8am. Additionally, the estimated count is high when the temperature is more than 22.25 degrees.

### Variable Importance in machine learning models
To investigate which variables affect the prediction most, we calculate the importance of the variables in the conditional inference tree and the random forest. In conditional inference tree, we calculated the mean decrease in accuracy when deleting a variable (Table 1).

```{r,echo=F}
knitr::kable(ctree_mda,caption = "Mean decrease in accuracy of variables in the conditional inference tree")
```

In random forest, we calculate the increase in MSE when deleting a variable (Figure 4). The code to do this was implemented as a function `plot_rf_importance` in the `bikeSharing` R package. 

```{r echo=TRUE, message=FALSE, warning=FALSE,fig.width=5,fig.height=4,fig.align='center',fig.cap="Feature importance in the random forest model"}
plot_rf_importance(london_train)
```

The top important variables of the conditional inference tree and random forest are similar. The hour chunk is the most important variable in both models. Additionally, the maximum temperature has a significant impact on the model prediction. Additional details of the variables are discussed in the discussion section. We further compare the performance of random forest and conditional inference tree using the second year bike renting data in London. Conditional inference tree results in a $R^2=0.81$ and random forest has $R^2=0.91$. Due the similarity in these two models, we selected random forest tree in the further comparison.



# Results 

```{r echo=TRUE}
glmm_model_fit(glmm_fit, london_train, scale_to_reference_mean = "no", 
               reference = london)
glmm_model_fit(glmm_fit, london_test, scale_to_reference_mean = "no",
               reference = london)
glmm_model_fit(glmm_fit, dc, scale_to_reference_mean = "yes",
               reference = london)
glmm_model_fit(glmm_fit, seoul, scale_to_reference_mean = "yes",
               reference = london)
```


```{r echo=TRUE}
rf_model_fit(rf_fit, london_train, scale_to_reference_mean = "no", 
             reference = london)
rf_model_fit(rf_fit, london_test, scale_to_reference_mean = "no", 
             reference = london)
rf_model_fit(rf_fit, dc, scale_to_reference_mean = "yes", 
             reference = london)
rf_model_fit(rf_fit, seoul, scale_to_reference_mean = "yes", 
             reference = london)
```



```{r message=FALSE, warning=FALSE}
seoul$city = "Seoul - Scaled"
london_train$city = "London"
london_test$city = "London"
dc$city = "DC - Scaled"
seoul$city2 = "Seoul - Scaled"
london_train$city2 = "London - Train"
london_test$city2 = "London - Test"
dc$city2 = "DC - Scaled"
scale_seoul = mean(seoul$Bike_count)/
  mean(london$Bike_count)
scale_dc = mean(dc$Bike_count)/
  mean(london$Bike_count)
seoul$Bike_count2 = seoul$Bike_count / scale_seoul
london_test$Bike_count2 = london_test$Bike_count 
london_train$Bike_count2 = london_train$Bike_count 
dc$Bike_count2 = dc$Bike_count / scale_dc
seoul$Year = 1
all_data = rbind(seoul, london_train, london_test, dc)
all_data$city = factor(all_data$city, 
                       levels = c("London",  "DC - Scaled", "Seoul - Scaled"))
all_data$city2 = factor(all_data$city2, 
                        levels = c("London - Train", "London - Test", 
                                   "DC - Scaled", "Seoul - Scaled"))
ggplot(all_data, aes(x = Bike_count2)) + 
  geom_histogram(fill = "dark blue", aes(y = stat(density))) + 
  facet_wrap(~city) + theme_classic() + xlim(0,30000) + 
  labs(title = "Distribution of Bike Counts for Cities", 
       y = "Density", x = "Bike Count")
ggplot(all_data, aes(x = Bike_count2)) + 
  geom_histogram(fill = "dark blue", aes(y = stat(density))) + 
  facet_wrap(~city2) + theme_classic() + xlim(0,30000) + 
  labs(title = "Distribution of Bike Counts for Cities", 
       y = "Density", x = "Bike Count")
mean(london_test$Bike_count)
mean(london_train$Bike_count)
```


# Discussion and Conclusion

In conclusion, factors including hours, holidays, min temperature, max temperature, min humidity, max humidity, average wind speed and presence of rain/snow all have inference on the number of bike rented in the three cities. Hours from 4pm to midnight have the largest effect on the number of bike rented. Hours from 8am to 4pm, winter season, rain/snow also have large impact on the bike rent count. Random Forest have better prediction result on all the training and testing data, compared to the results of GLMM.

In addition to the conclusion above, we also have some findings that need to discuss more. The RMSE for Seoul is much larger than that in the other two cities. The results on the test dataset are much worse than in the training dataset. This may imply that our model may have some limitations across cities. The possible reason is that in our model, we only considered the effects of days, hours and weather on the bike count but did not take information about cities into consideration (e.g. population, GDP), which may cause bias in prediction.

To get a better understanding on how our model predict on the sequences of bike count in different dataset, we plotted out the predicted bike count and the ground truth for each of the four datasets. Below is the longitudinal plots of the predicted bike count and the true bike count in the three cities.

```{r,include=FALSE}
# function to output the prediction results
get_prediction = function(model_glmm=output,data,scale_data=FALSE) {
  data = data[data$Date %in% london$Date,]
  ## Name day ranef from seoul
  names(model_glmm$day_ranef) = unique(london$Date)
  ## Predict log Mean for each day Values
  model_matrix = model.matrix( Bike_count ~ Hour_chunks + Is_weekend + Is_holiday + Season + Min_temp + Max_temp + Min_humidity + Max_humidity + Wind_speed + Rain_or_snow, data = data)
  fixedef = c(model_matrix %*% model_glmm$beta)
  ranef = model_glmm$day_ranef[match(data$Date,names(model_glmm$day_ranef))]
  logmeans = fixedef + ranef
  ## Transform to get prediction
  pred = exp(logmeans)
  if (scale_data) {
    scale = mean(data$Bike_count)/mean(london$Bike_count)
    pred = pred * scale
  }
  pred = as.numeric(pred)
  true = as.numeric(data$Bike_count)
  df = as.data.frame(cbind(index = c(1:length(pred)),pred = pred, true = true))
}
```

```{r,include=FALSE}
load("londonoutput_M1000.Rdata")
model_glmm = output

df_seoul = get_prediction(model_glmm=model_glmm,data=seoul,scale_data=TRUE)

df_london_train = get_prediction(model_glmm=model_glmm,data=london_train,scale_data=FALSE)

df_london_test = get_prediction(model_glmm=model_glmm,data=london_test,scale_data=FALSE)

df_dc = get_prediction(model_glmm=model_glmm,data=dc,scale_data=TRUE)
```

```{r, warning=FALSE,fig.align='center',message=FALSE}
#install.packages("ggpubr")
library(ggpubr)
library(ggplot2)
p1=
  ggplot(data = df_seoul,aes(index))+geom_point(aes(y=pred),size=0.2)+geom_line(aes(y=pred),size=0.2)+
  geom_point(aes(y=true),color = "red",alpha=0.4,size=0.2)+geom_line(aes(y=true),color = "red",alpha=0.5,size=0.1)+
  ggtitle("Bike Count in Seoul Pred (black) vs True") + ylim(0,35000) +xlab("Time Index")+
  theme(plot.title=element_text(size=8))

p2=
  ggplot(data = df_london_train,aes(index))+geom_point(aes(y=pred),size=0.2)+geom_line(aes(y=pred),size=0.2)+
  geom_point(aes(y=true),color = "red",alpha=0.4,size=0.2)+geom_line(aes(y=true),color = "red",alpha=0.5,size=0.1)+
  ggtitle("Bike Count in London Train Pred (black) vs True")+ ylim(0,35000)+xlab("Time Index")+
  theme(plot.title=element_text(size=8))

p3=
  ggplot(data = df_london_test,aes(index))+geom_point(aes(y=pred),size=0.2)+geom_line(aes(y=pred),size=0.2)+
  geom_point(aes(y=true),color = "red",alpha=0.4,size=0.2)+geom_line(aes(y=true),color = "red",alpha=0.5,size=0.1)+
  ggtitle("Bike Count in London Test Pred (black) vs True")+ ylim(0,35000)+xlab("Time Index")+
  theme(plot.title=element_text(size=8))

p4=
  ggplot(data = df_dc,aes(index))+geom_point(aes(y=pred),size=0.2)+geom_line(aes(y=pred),size=0.2)+
  geom_point(aes(y=true),color = "red",alpha=0.4,size=0.2)+geom_line(aes(y=true),color = "red",alpha=0.5,size=0.1)+
  ggtitle("Bike Count in DC Pred (black) vs True")+ ylim(0,35000)+xlab("Time Index")+
  theme(plot.title=element_text(size=8))
ggarrange(p1, p2, p3, p4, 
          labels = c("A", "B", "C","D"),
          ncol = 2, nrow = 2)
#dev.off()

```

To understand the bias and variance of our prediction results, we also drew the boxplots of predicted bike count and the true bike count of the four datasets. The plots are shown below:

```{r, warning=FALSE,fig.align='center'}
library(tidyverse)
library(dplyr)
boxplot_seoul = df_seoul %>% 
  pivot_longer(
    cols = c("pred", "true"), 
    names_to = "label", 
    values_to = "value"
  )
p1 = ggplot(data = boxplot_seoul,aes(x = label, y = value)) +geom_boxplot()+ylim(0,35000)+
  ggtitle("Bike Count Boxplot in Seoul Pred (left) vs True")+ylab("Bike Count")+
  theme(plot.title=element_text(size=8))

boxplot_london_train = df_london_train %>% 
  pivot_longer(
    cols = c("pred", "true"), 
    names_to = "label", 
    values_to = "value"
  )
p2=ggplot(data = boxplot_london_train,aes(x = label, y = value)) +geom_boxplot() + ylim(0,35000)+
  ggtitle("Bike Count Boxplot in London Train Pred (left) vs True")+ylab("Bike Count")+
  theme(plot.title=element_text(size=8))

boxplot_london_test = df_london_test %>% 
  pivot_longer(
    cols = c("pred", "true"), 
    names_to = "label", 
    values_to = "value"
  )
p3=ggplot(data = boxplot_london_test,aes(x = label, y = value)) +geom_boxplot() + ylim(0,35000)+
  ggtitle("Bike Count Boxplot in London Test Pred (left) vs True")+ylab("Bike Count")+
  theme(plot.title=element_text(size=8))

boxplot_dc = df_dc %>% 
  pivot_longer(
    cols = c("pred", "true"), 
    names_to = "label", 
    values_to = "value"
  )
p4=ggplot(data = boxplot_dc,aes(x = label, y = value)) +geom_boxplot() + ylim(0,35000)+
  ggtitle("Bike Count Boxplot in DC Test Pred (left) vs True")+ylab("Bike Count")+
  theme(plot.title=element_text(size=8))
ggarrange(p1, p2, p3, p4, 
          labels = c("A", "B", "C","D"),
          ncol = 2, nrow = 2)
```

Overall, our result is consistent with the published research by Sylwia et al. (2021) in "Impact of environment on bicycle travel demand-Assessment using bikeshare system data". In their study, they used the data from the bike sharing system in Cracow, Polan and conducted an ordinal least square regression model to analyze the effect of daily air temperature, daily rainfall, public holidays, and school holidays on the daily number of bike rented from bike sharing system. Their study result indicated that weather conditions, especially air temperature and daily rainfall, have large impact on the number of bike sharing system, which is consistent with ours result. Compared to their research, our study further found that the maximum temperature and the minimum humidity have more impact on the bike count. We did not restrict our study on daily level but cut one day into different hour chunks and found that different hour chunks in one day can also have large effect on the number of bike rented from bike sharing system.


# Reference
Hothorn T, Hornik K, Zeileis A (2006). “Unbiased Recursive Partitioning: A Conditional
Inference Framework.” Journal of Computational and Graphical Statistics, 15(3), 651–674.
doi:10.1198/106186006X133933.

Sylwia P., Mariusz K., Carmelo D (2021). "Impact of environment on bicycle travel demand—Assessment using bikeshare system data." Sustainable Cities and Society, 67, [102724]. https://doi.org/10.1016/j.scs.2021.102724

United Nations, Department of Economic and Social Affairs, Population Division (2018). World Urbanization Prospects: The 2018 Revision, Online Edition.